{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codened/DataStorm-4.0/blob/main/stormingRound/DataStorm_4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.dtreeg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx9iBCyQ2k9z"
      },
      "source": [
        "Path \n",
        "stormingRound/DataStorm_4_0.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wW7bJky-1JT"
      },
      "source": [
        "# Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79FVB5FI6Cs2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be7BxsqWn7E7"
      },
      "source": [
        "# Importing Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Oll2JxA5x9Y"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF=pd.read_csv('Historical-transaction-data.csv')\n",
        "rawStoreInfDF=pd.read_csv('Store-info.csv')\n",
        "rawTestDF=pd.read_csv('Testing-data.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Viewing Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtD8h2FiSA2r"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rawStoreInfDF.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pre Processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fixing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnrCJ0o-Sb8v"
      },
      "outputs": [],
      "source": [
        "# convert the date string column to datetime\n",
        "rawHisTransDF['transaction_date'] = pd.to_datetime(rawHisTransDF['transaction_date'], format='%Y/%m/%d').dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFqYaVSyjoBH"
      },
      "outputs": [],
      "source": [
        "# Performing left join\n",
        "merged_df = pd.merge(rawHisTransDF, rawStoreInfDF, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "Rb8HF5A2S9eg",
        "outputId": "498e142e-dc75-4bf8-f90a-fb5b8af35d0d"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF.describe(include='all').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get count of null values in each column\n",
        "null_counts = merged_df.isnull().sum()\n",
        "# print the counts\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df.dropna(subset=['item_description','invoice_id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get count of null values in each column\n",
        "null_counts = merged_df.isnull().sum()\n",
        "# print the counts\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "merged_df['item_description'] = le.fit_transform(merged_df['item_description'])\n",
        "merged_df['customer_id'] = le.fit_transform(merged_df['customer_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['shop_id'] = merged_df['shop_id'].str.replace(r'^SHOP', '').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['shop_profile'] = merged_df['shop_profile'].replace({'High': 3, 'Moderate': 2, 'Low': 1})\n",
        "merged_df['shop_profile'] = merged_df['shop_profile'].fillna(0.0).astype(int)\n",
        "merged_df['invoice_id'] = merged_df['invoice_id'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(merged_df[merged_df['quantity_sold'] == 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = merged_df[merged_df['quantity_sold'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['full_price'] = merged_df['quantity_sold'] * merged_df['item_price']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### creating Avarage daily sales for each shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['Daily_Sales'] = merged_df.groupby(['shop_id', 'transaction_date'])['full_price'].transform('sum')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subset = merged_df.loc[(merged_df['transaction_date'] == pd.to_datetime('2021-12-11')) & (merged_df['shop_id'] == 8)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by shop id and calculate mean of daily_sales column\n",
        "avg_sales = merged_df.groupby('shop_id')['Daily_Sales'].mean().reset_index()\n",
        "\n",
        "# Merge the average sales data back into the original dataframe\n",
        "merged_df = merged_df.merge(avg_sales, on='shop_id', suffixes=('', '_avg'))\n",
        "\n",
        "# Print the updated dataframe\n",
        "merged_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Full revinew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['revnew'] = merged_df.groupby(['shop_id'])['full_price'].transform('sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Revnew per sqr feet of land"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['rev_per_sqfeet'] = (merged_df['revnew'] / merged_df['shop_area_sq_ft']).round().astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avarage sold item types per each shop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# group the original table by Shop ID and Transaction Date and count the unique items sold on each day\n",
        "daily_items_sold = merged_df.groupby(['shop_id', 'transaction_date'])['item_description'].nunique().reset_index()\n",
        "\n",
        "# group the resulting table by Shop ID and take the mean of the nunique column\n",
        "avg_daily_items_sold = daily_items_sold.groupby('shop_id')['item_description'].mean().reset_index()\n",
        "\n",
        "# rename the columns\n",
        "avg_daily_items_sold.columns = ['shop_id', 'avd_daily_items_types_sold']\n",
        "# convert float column to integers\n",
        "avg_daily_items_sold['avd_daily_items_types_sold'] = avg_daily_items_sold['avd_daily_items_types_sold'].round().astype(int)\n",
        "\n",
        "# merge with the original dataframe\n",
        "merged_df = pd.merge(merged_df, avg_daily_items_sold, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avarage Daily Transactions per each shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# group the original table by Shop ID and Transaction Date and count the unique items sold on each day\n",
        "daily_trans = merged_df.groupby(['shop_id', 'transaction_date'])['invoice_id'].nunique().reset_index()\n",
        "\n",
        "# group the resulting table by Shop ID and take the mean of the nunique column\n",
        "avg_daily_trans = daily_trans.groupby('shop_id')['invoice_id'].mean().reset_index()\n",
        "\n",
        "# rename the columns\n",
        "avg_daily_trans.columns = ['shop_id', 'avd_daily_transctions']\n",
        "# convert float column to integers\n",
        "avg_daily_trans['avd_daily_transctions'] = avg_daily_trans['avd_daily_transctions'].round().astype(int)\n",
        "\n",
        "# merge with the original dataframe\n",
        "merged_df = pd.merge(merged_df, avg_daily_trans, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Average number of custemers per day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# group the original table by Shop ID and Transaction Date and count the unique items sold on each day\n",
        "daily_custemers = merged_df.groupby(['shop_id', 'transaction_date'])['customer_id'].nunique().reset_index()\n",
        "\n",
        "# group the resulting table by Shop ID and take the mean of the nunique column\n",
        "avg_daily_custemers = daily_custemers.groupby('shop_id')['customer_id'].mean().reset_index()\n",
        "\n",
        "# rename the columns\n",
        "avg_daily_custemers.columns = ['shop_id', 'avd_daily_custemers']\n",
        "# convert float column to integers\n",
        "avg_daily_custemers['avd_daily_custemers'] = avg_daily_custemers['avd_daily_custemers'].round().astype(int)\n",
        "\n",
        "# merge with the original dataframe\n",
        "merged_df = pd.merge(merged_df, avg_daily_custemers, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Persentage of Avarage number of time the same customer returning for the same shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the number of times each customer visited each shop\n",
        "visits = merged_df.groupby(['customer_id', 'shop_id'])['transaction_date'].count()\n",
        "# calculate the average number of visits per customer per shop\n",
        "avg_visits = visits.groupby(['shop_id']).mean()*100\n",
        "\n",
        "avg_visits=avg_visits.round().astype(int)\n",
        "# create a new DataFrame with the average visits\n",
        "avg_visits_df = avg_visits.reset_index().rename(columns={'transaction_date': 'avg_visits'})\n",
        "\n",
        "# merge the new DataFrame with the original DataFrame to add the average visits column\n",
        "merged_df = pd.merge(merged_df, avg_visits_df, on=['shop_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avg price for each item in each shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for (shop_id, item_description), item_price in merged_df.groupby(['shop_id', 'item_description'])['item_price']:\n",
        "    print(f\"shop_id: {shop_id}, Item type: {item_description}\")\n",
        "    print(item_price)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate average price for each item type sold by each shop\n",
        "avg_prices = merged_df.groupby(['shop_id', 'item_description'])['item_price'].mean().reset_index()\n",
        "\n",
        "# Rename 'price' column to 'avg_price'\n",
        "avg_prices = avg_prices.rename(columns={'item_price': 'avg_price'})\n",
        "\n",
        "# Merge the average prices back into the original DataFrame\n",
        "merged_df = pd.merge(merged_df, avg_prices, on=['shop_id', 'item_description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Number of items each item sold by each shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the number of times each item sold by each shop\n",
        "Num_of_times = merged_df.groupby(['item_description', 'shop_id'])['quantity_sold'].transform('sum')\n",
        "\n",
        "Num_of_times=Num_of_times.to_frame()\n",
        "\n",
        "Num_of_times = Num_of_times.rename(columns={'quantity_sold': 'num_of_times_item_sold'})\n",
        "\n",
        "\n",
        "# concatenate the dataframes\n",
        "merged_df = pd.concat([merged_df, Num_of_times], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Total quantity sold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tot_quantity_sold= merged_df.groupby(['shop_id'])['quantity_sold'].transform('sum')\n",
        "\n",
        "tot_quantity_sold=tot_quantity_sold.to_frame()\n",
        "\n",
        "tot_quantity_sold = tot_quantity_sold.rename(columns={'quantity_sold': 'tot_quantity_sold'})\n",
        "\n",
        "# concatenate the dataframes\n",
        "merged_df = pd.concat([merged_df, tot_quantity_sold], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Average Item Price Per Shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['avg_itm_pp_Shop'] = (merged_df['revnew'] / merged_df['tot_quantity_sold']).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "corr = merged_df.corr()\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot correlation matrix as heatmap\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "\n",
        "# Display plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop highly co related features\n",
        "cleanedDF = merged_df.drop(['avd_daily_custemers','transaction_date','revnew','item_price','item_description','quantity_sold','full_price','customer_id','avd_daily_transctions'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Drop all low MI features\n",
        "# cleanedDF = cleanedDF.drop(['avd_daily_items_types_sold','num_of_times_item_sold','avg_visits','shop_area_sq_ft','invoice_id','avg_itm_pp_Shop'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop duplicates\n",
        "cleanedDF.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanedDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "corr = cleanedDF.corr()\n",
        "\n",
        "# # Set figure size\n",
        "# plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot correlation matrix as heatmap\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "\n",
        "# Display plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Micro Data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the number of times each item sold by each shop\n",
        "\n",
        "avg_itm_pp_Shop = merged_df.groupby(['shop_id', 'item_description']).agg({'item_description': 'mean', 'avg_itm_pp_Shop': 'mean'})\n",
        "\n",
        "# Rename columns\n",
        "avg_itm_pp_Shop.columns = ['item_description', 'avg_itm_pp_Shop']\n",
        "\n",
        "# Reset index to create a new dataframe\n",
        "avg_itm_pp_Shop = avg_itm_pp_Shop.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_itm_pp_Shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanedDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a new smaller dataframe by grouping all the rows by shop_id and taking the mean of each column exept shop profile column to the corresponding shop id column\n",
        "microDF=cleanedDF.groupby('shop_id').mean().reset_index().drop(['shop_profile','invoice_id','Daily_Sales'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "profiles=cleanedDF[['shop_id','shop_profile']].drop_duplicates().sort_values(by=['shop_id']).reset_index().drop(['index'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "profiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "microDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "microDF=(pd.merge(microDF, profiles, on='shop_id', how='left')).round(0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "microDF"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split To Test and Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the DataFrame into two based on column B\n",
        "Eval_mic_DF = microDF[microDF['shop_profile'] == 0].drop(['shop_profile'], axis=1).reset_index().drop(['index'], axis=1)\n",
        "Train_mic_DF = microDF[microDF['shop_profile'] != 0].reset_index().drop(['index'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Train_mic_DF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Separate the target variable\n",
        "X = Train_mic_DF.drop(['shop_profile'], axis=1)\n",
        "y = Train_mic_DF['shop_profile']\n",
        "\n",
        "# Compute MI scores\n",
        "mi_scores = mutual_info_classif(X, y)\n",
        "\n",
        "# Convert to DataFrame and sort by MI score\n",
        "mi_scores_df = pd.DataFrame({'feature': X.columns, 'mi_score': mi_scores})\n",
        "mi_scores_df = mi_scores_df.sort_values('mi_score', ascending=False)\n",
        "\n",
        "# Plot bar chart of MI scores\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(mi_scores_df['feature'], mi_scores_df['mi_score'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('MI Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "corr = microDF.corr()\n",
        "\n",
        "# # Set figure size\n",
        "# plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot correlation matrix as heatmap\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "\n",
        "# Display plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the dataset into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data_mic, test_data_mic = train_test_split(Train_mic_DF, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data_mic=test_data_mic.reset_index(drop=True)\n",
        "train_data_mic=train_data_mic.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove store id from the training and testing sets\n",
        "\n",
        "test_data_mic_noID = test_data_mic.drop(['shop_id'], axis=1)\n",
        "train_data_mic_noID = train_data_mic.drop(['shop_id'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train=train_data_mic_noID.drop('shop_profile', axis=1)\n",
        "y_train=train_data_mic_noID['shop_profile']\n",
        "X_test=test_data_mic_noID.drop('shop_profile', axis=1)\n",
        "y_test=test_data_mic_noID['shop_profile']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decition tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "\n",
        "# Define the decision tree classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "params = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [2, 3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 3, 4, 5],\n",
        "    'min_samples_leaf': [1, 2, 3, 4],\n",
        "}\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search = GridSearchCV(dtc, param_grid=params, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Print the best hyperparameters and the corresponding score\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Train the Decision Trees model\n",
        "dt_model = DecisionTreeClassifier() # criterion= 'gini', max_depth= 3, min_samples_leaf= 3, min_samples_split= 5)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Test the model on the testing set\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# calculate the F1 score for each class\n",
        "f1_class2 = f1_score(y_test, y_pred, labels=[2], average='weighted')\n",
        "f1_class1 = f1_score(y_test, y_pred, labels=[1], average='weighted')\n",
        "f1_class0 = f1_score(y_test, y_pred, labels=[0], average='weighted')\n",
        "\n",
        "# calculate the average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "# print the results\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")\n",
        "\n",
        "formatted_f1_average = \"{:.4f}\".format(f1_average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assume y_true and y_pred are the true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap of the confusion matrix\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Big Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_mic_noID['shop_profile'] = train_data_mic_noID['shop_profile'].replace({1: 0, 2: 1, 3: 2})\n",
        "test_data_mic_noID['shop_profile'] = test_data_mic_noID['shop_profile'].replace({1: 0, 2: 1, 3: 2})\n",
        "test_data_mic['shop_profile'] = test_data_mic['shop_profile'].replace({1: 0, 2: 1, 3: 2})\n",
        "train_data_mic['shop_profile'] = train_data_mic['shop_profile'].replace({1: 0, 2: 1, 3: 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train=train_data_mic_noID.drop('shop_profile', axis=1)\n",
        "y_train=train_data_mic_noID['shop_profile']\n",
        "X_test=test_data_mic.drop('shop_profile', axis=1)\n",
        "y_test=test_data_mic['shop_profile']\n",
        "\n",
        "# Define the models to be hyperparameter tuned\n",
        "models = [    {'name': 'XGBoost',        \n",
        "            'model': XGBClassifier(tree_method='gpu_hist'),        \n",
        "            'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.3],\n",
        "            'min_child_weight': [1, 3, 5],\n",
        "            'gamma': [0.0, 0.1, 0.2]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Random Forest',\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['sqrt', 'log2']\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Logistic Regression',\n",
        "        'model': LogisticRegression(penalty='l2'),\n",
        "        'params': {\n",
        "            'C': [0.1, 1.0, 10.0],\n",
        "            'solver': ['lbfgs', 'liblinear', 'saga'],\n",
        "            'max_iter': [100, 500, 1000]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'KNN',\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'n_neighbors': [3, 5, 7],\n",
        "            'p': [1, 2],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "# Perform hyperparameter tuning for each model using GridSearchCV\n",
        "best_model = None\n",
        "best_score = 0.0\n",
        "for model_config in models:\n",
        "    print(f'Tuning {model_config[\"name\"]}...')\n",
        "    model = model_config['model']\n",
        "    params = model_config['params']\n",
        "    # Create scorer object using custom scoring function\n",
        "    # scorer = make_scorer(custom_scorer)\n",
        "\n",
        "    custom_grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=params,\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    custom_grid_search.fit(X_train, y_train)\n",
        "    score = custom_grid_search.best_score_\n",
        "    print(f'Best score for {model_config[\"name\"]}: {score:.4f}')\n",
        "    print(f'Best Parametersfor {model_config[\"name\"]}:  {custom_grid_search.best_params_}')\n",
        "    \n",
        "    \n",
        "    \n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_model = custom_grid_search.best_estimator_\n",
        "        best_model_name = model_config['name']\n",
        "        best_model_hyperparams = custom_grid_search.best_params_\n",
        "\n",
        "# Train the best model on the full training set\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy score on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Test accuracy score for the best model: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_mic_noID['shop_profile'] = train_data_mic_noID['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "test_data_mic_noID['shop_profile'] = test_data_mic_noID['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "test_data_mic['shop_profile'] = test_data_mic['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "train_data_mic['shop_profile'] = train_data_mic['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "y_pred['shop_profile'] = y_pred['shop_profile'].replace({0: 1, 1: 2, 2: 3})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Huge data Set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split To Test and Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the DataFrame into two based on column B\n",
        "TestDF = cleanedDF[cleanedDF['shop_profile'] == 0].drop(['shop_profile'], axis=1)\n",
        "TrainDF = cleanedDF[cleanedDF['shop_profile'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reset index\n",
        "TestDF=TestDF.reset_index(drop=True)\n",
        "TrainDF=TrainDF.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Separate the target variable\n",
        "X = TrainDF.drop(['shop_profile'], axis=1)\n",
        "y = TrainDF['shop_profile']\n",
        "\n",
        "# Compute MI scores\n",
        "mi_scores = mutual_info_classif(X, y)\n",
        "\n",
        "# Convert to DataFrame and sort by MI score\n",
        "mi_scores_df = pd.DataFrame({'feature': X.columns, 'mi_score': mi_scores})\n",
        "mi_scores_df = mi_scores_df.sort_values('mi_score', ascending=False)\n",
        "\n",
        "# Plot bar chart of MI scores\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(mi_scores_df['feature'], mi_scores_df['mi_score'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('MI Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# # Create principal components\n",
        "# pca = PCA()\n",
        "# PCA_X = pca.fit_transform(X.drop(['shop_id'], axis=1))\n",
        "\n",
        "# # Convert to dataframe\n",
        "# component_names = [f\"PC{i+1}\" for i in range(PCA_X.shape[1])]\n",
        "# PCA_X = pd.DataFrame(PCA_X, columns=component_names)\n",
        "\n",
        "# PCA_X = pd.concat([PCA_X, X['shop_id']], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.feature_selection import mutual_info_classif\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Compute MI scores\n",
        "# mi_scores = mutual_info_classif(PCA_X, y)\n",
        "\n",
        "# # Convert to DataFrame and sort by MI score\n",
        "# mi_scores_df = pd.DataFrame({'feature': PCA_X.columns, 'mi_score': mi_scores})\n",
        "# mi_scores_df = mi_scores_df.sort_values('mi_score', ascending=False)\n",
        "\n",
        "# # Plot bar chart of MI scores\n",
        "# plt.figure(figsize=(12,8))\n",
        "# plt.bar(mi_scores_df['feature'], mi_scores_df['mi_score'])\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.xlabel('Feature')\n",
        "# plt.ylabel('MI Score')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X=PCA_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanedDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split Fulldata into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "column_name = 'shop_id'\n",
        "unique_categories = TrainDF[column_name].nunique()\n",
        "categories_in_dataset_1 = int(unique_categories * 0.8)\n",
        "categories_in_dataset_2 = unique_categories - categories_in_dataset_1\n",
        "dataset_1_categories = TrainDF[column_name].unique()[:categories_in_dataset_1]\n",
        "dataset_2_categories = TrainDF[column_name].unique()[categories_in_dataset_1:]\n",
        "\n",
        "train_data = TrainDF[TrainDF[column_name].isin(dataset_1_categories)]\n",
        "test_data = TrainDF[TrainDF[column_name].isin(dataset_2_categories)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#train_data, test_data = train_test_split(TrainDF, test_size=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data=test_data.reset_index(drop=True)\n",
        "train_data=train_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove store id from the training and testing sets\n",
        "\n",
        "train_data_noID = train_data.drop(['shop_id'], axis=1)\n",
        "test_data_noID = test_data.drop(['shop_id'], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XG boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_noID['shop_profile'] = train_data_noID['shop_profile'].replace({1: 0, 2: 1, 3: 2})\n",
        "test_data_noID['shop_profile'] = test_data_noID['shop_profile'].replace({1: 0, 2: 1, 3: 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# import xgboost as xgb\n",
        "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "# # Split data into training and test sets\n",
        "# X_train=train_data_noID.drop('shop_profile', axis=1)\n",
        "# y_train=train_data_noID['shop_profile']\n",
        "# X_test=test_data_noID.drop('shop_profile', axis=1)\n",
        "# y_test=test_data_noID['shop_profile']\n",
        "\n",
        "# # Set the parameters for grid search\n",
        "# params = {\n",
        "#     'n_estimators': [100, 500, 1000],\n",
        "#     'learning_rate': [0.01, 0.1, 1],\n",
        "#     'max_depth': [3, 5, 7],\n",
        "#     'subsample': [0.5, 0.75, 1],\n",
        "#     'colsample_bytree': [0.5, 0.75, 1],\n",
        "#     'objective': ['multi:softmax', 'multi:softprob'],\n",
        "#     'num_class': [3],\n",
        "#     'tree_method': ['gpu_hist']\n",
        "# }\n",
        "\n",
        "# # Initialize the XGBoost classifier\n",
        "# xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "# # Perform grid search to find the best hyperparameters\n",
        "# grid_search = GridSearchCV(estimator=xgb_model, param_grid=params, scoring='accuracy', cv=3, n_jobs=-1, verbose=1)\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Print the best hyperparameters found by grid search\n",
        "# print(grid_search.best_params_)\n",
        "\n",
        "# # Train the model using the best hyperparameters found by grid search\n",
        "# xgb_model = xgb.XGBClassifier(**grid_search.best_params_)\n",
        "# xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions on the test set\n",
        "# xg_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# # Evaluate the model's performance on the test set\n",
        "# accuracy = np.mean(xg_pred == y_test)\n",
        "# print('Accuracy:', accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best hyperparameters:  {'subsample': 1.0, 'reg_lambda': 0, 'reg_alpha': 0.1, 'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.01, 'gamma': 0.5, 'colsample_bytree': 0.5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train=train_data_noID.drop('shop_profile', axis=1)\n",
        "y_train=train_data_noID['shop_profile']\n",
        "X_test=test_data_noID.drop('shop_profile', axis=1)\n",
        "y_test=test_data_noID['shop_profile']\n",
        "\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(tree_method='gpu_hist')\n",
        "\n",
        "# fit model to training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on test data\n",
        "xg_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# evaluate performance of model\n",
        "mse = mean_squared_error(y_test, xg_pred)\n",
        "print('MSE:', mse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MSE: 0.40624200562803786"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xg_pred=pd.DataFrame(xg_pred, columns=['shop_profile'])\n",
        "\n",
        "train_data_noID['shop_profile'] = train_data_noID['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "test_data_noID['shop_profile'] = test_data_noID['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "xg_pred['shop_profile'] = xg_pred['shop_profile'].replace({0: 1, 1: 2, 2: 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predicted_res = pd.concat([test_data['shop_id'], pred['shop_profile']], axis=1)\n",
        "# expected_res=test_data[['shop_id', 'shop_profile']]\n",
        "\n",
        "# pred_mode = predicted_res.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()\n",
        "# exp_mode = expected_res.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()\n",
        "\n",
        "# # import necessary libraries\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.metrics import f1_score\n",
        "\n",
        "# y_test=exp_mode['shop_profile']\n",
        "# y_pred=pred_mode['shop_profile']\n",
        "\n",
        "# # calculate the F1 score for each class\n",
        "# f1_class0 = f1_score(y_test, y_pred, labels=[1], average='weighted')\n",
        "# f1_class1 = f1_score(y_test, y_pred, labels=[2], average='weighted')\n",
        "# f1_class2 = f1_score(y_test, y_pred, labels=[3], average='weighted')\n",
        "\n",
        "# # calculate the average F1 score\n",
        "# f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "# # print the results\n",
        "# print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "# print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "# print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "# print(f\"Average F1 score: {f1_average:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xg_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_df_XG_res = pd.concat([test_data['shop_id'], xg_pred['shop_profile']], axis=1)\n",
        "# concatenated_df_XG_res['shop_profile'] = concatenated_df_XG_res['shop_profile'].astype(int)\n",
        "# concatenated_df_XG_res['shop_id'] = concatenated_df_XG_res['shop_id'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_df_XG_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expected_df_XG=test_data[['shop_id', 'shop_profile']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expected_df_XG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# group by 'group' column and calculate mode of 'value' column\n",
        "XG_res_mode_df = concatenated_df_XG_res.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XG_res_mode_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# group by 'group' column and calculate mode of 'value' column\n",
        "XG_exp_mode_df = expected_df_XG.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "XG_exp_mode_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_test=XG_exp_mode_df['shop_profile']\n",
        "y_pred=XG_res_mode_df['shop_profile']\n",
        "\n",
        "# calculate the F1 score for each class\n",
        "f1_class0 = f1_score(y_test, y_pred, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(y_test, y_pred, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(y_test, y_pred, labels=[3], average='weighted')\n",
        "\n",
        "# calculate the average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "# print the results\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Average F1 score: 0.47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assume y_true and y_pred are the true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap of the confusion matrix\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forrest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train=train_data_noID.drop('shop_profile', axis=1)\n",
        "y_train=train_data_noID['shop_profile']\n",
        "X_test=test_data_noID.drop('shop_profile', axis=1)\n",
        "y_test=test_data_noID['shop_profile']\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rfc = RandomForestClassifier(max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=25)\n",
        "\n",
        "# Train the model on the training data\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "RF_pred = rfc.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, RF_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accuracy: 0.6433444188624542"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# # Split data into training and test sets\n",
        "# X_train=train_data_noID.drop('shop_profile', axis=1)\n",
        "# y_train=train_data_noID['shop_profile']\n",
        "# X_test=test_data_noID.drop('shop_profile', axis=1)\n",
        "# y_test=test_data_noID['shop_profile']\n",
        "\n",
        "# # Define the parameter grid to search over\n",
        "# param_grid = {\n",
        "#     'n_estimators': [50, 100, 150, 200],\n",
        "#     'max_depth': [10, 20, 30, None],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4],\n",
        "#     'max_features': ['auto', 'sqrt']\n",
        "# }\n",
        "\n",
        "# # Create the Random Forest classifier\n",
        "# rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# # Perform a grid search over the parameter grid with cross-validation\n",
        "# rf_cv = GridSearchCV(rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# # Fit the grid search to the training data\n",
        "# rf_cv.fit(X_train, y_train)\n",
        "\n",
        "# # Print the best parameters found by the grid search\n",
        "# print(\"Best Parameters:\", rf_cv.best_params_)\n",
        "\n",
        "# # Predict on the test data using the best model\n",
        "# RF_pred = rf_cv.predict(X_test)\n",
        "\n",
        "# # Print the classification report\n",
        "# print(classification_report(y_test, RF_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RF_pred = pd.DataFrame(RF_pred, columns=['shop_profile'])\n",
        "\n",
        "predicted_res = pd.concat([test_data['shop_id'], RF_pred['shop_profile']], axis=1)\n",
        "expected_res=test_data[['shop_id', 'shop_profile']]\n",
        "\n",
        "pred_mode = predicted_res.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()\n",
        "exp_mode = expected_res.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()\n",
        "\n",
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_test=exp_mode['shop_profile']\n",
        "y_pred=pred_mode['shop_profile']\n",
        "\n",
        "# calculate the F1 score for each class\n",
        "f1_class0 = f1_score(y_test, y_pred, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(y_test, y_pred, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(y_test, y_pred, labels=[3], average='weighted')\n",
        "\n",
        "# calculate the average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "# print the results\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Average F1 score: 0.53"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assume y_true and y_pred are the true and predicted labels, respectively\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap of the confusion matrix\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Eval_noID=TestDF.drop('shop_id', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Eval_noID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_Eval=Eval_noID\n",
        "\n",
        "# Predict on the evaluation set\n",
        "RF_eval_pred = rfc.predict(X_Eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RF_eval_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RF_eval_pred = pd.DataFrame(RF_eval_pred, columns=['shop_profile'])\n",
        "\n",
        "predicted_eval_res = pd.concat([TestDF['shop_id'], RF_eval_pred['shop_profile']], axis=1)\n",
        "\n",
        "\n",
        "pred_Eval_mode = predicted_eval_res.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_Eval_mode['shop_profile'] = pred_Eval_mode['shop_profile'].replace({1:'High', 2:'Moderate', 3:'Low'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_Eval_mode"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Big Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_data_noID['shop_profile'] = train_data_noID['shop_profile'].replace({1: 0, 2: 1, 3: 2})\n",
        "# test_data_noID['shop_profile'] = test_data_noID['shop_profile'].replace({1: 0, 2: 1, 3: 2})\n",
        "# test_data['shop_profile'] = test_data['shop_profile'].replace({1: 0, 2: 1, 3: 2})\n",
        "# train_data['shop_profile'] = train_data['shop_profile'].replace({1: 0, 2: 1, 3: 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.datasets import make_classification\n",
        "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "# from sklearn.metrics import make_scorer, accuracy_score\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "\n",
        "# # # Create a custom scoring function\n",
        "# # def custom_scorer(y_true, y_pred):\n",
        "# #     selected_testData = test_data.loc[y_pred.index]\n",
        "# #     # predicted_res = pd.concat([test_data['shop_id'], y_pred['shop_profile']], axis=1)\n",
        "# #     # merge dataframes on row index\n",
        "# #     predicted_res = y_pred['shop_profile'].merge(test_data['shop_id'], left_index=True, right_index=True, how='left')\n",
        "# #     expected_res=selected_testData[['shop_id', 'shop_profile']]\n",
        "\n",
        "# #     # reset indexes\n",
        "# #     predicted_res=predicted_res.reset_index(drop=True)\n",
        "# #     expected_res=expected_res.reset_index(drop=True)\n",
        "    \n",
        "# #     predicted_res\n",
        "# #     expected_res.head(10)\n",
        "    \n",
        "# #     pred_mode = predicted_res.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()\n",
        "# #     exp_mode = expected_res.groupby('shop_id')['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()\n",
        "    \n",
        "# #     pred_mode.head(10)\n",
        "# #     exp_mode.head(10)\n",
        "\n",
        "# #     # import necessary libraries\n",
        "# #     import pandas as pd\n",
        "# #     from sklearn.model_selection import train_test_split\n",
        "# #     from sklearn.tree import DecisionTreeClassifier\n",
        "# #     from sklearn.metrics import f1_score\n",
        "\n",
        "# #     y_test=exp_mode['shop_profile']\n",
        "# #     y_pred=pred_mode['shop_profile']\n",
        "\n",
        "# #     # calculate the F1 score for each class\n",
        "# #     f1_class0 = f1_score(y_test, y_pred, labels=[0], average='weighted')\n",
        "# #     f1_class1 = f1_score(y_test, y_pred, labels=[1], average='weighted')\n",
        "# #     f1_class2 = f1_score(y_test, y_pred, labels=[2], average='weighted')\n",
        "\n",
        "# #     # calculate the average F1 score\n",
        "# #     f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "# #     # print the results\n",
        "# #     print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "# #     print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "# #     print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "# #     print(f\"Average F1 score: {f1_average:.2f}\")\n",
        "    \n",
        "# #     formatted_f1_average = \"{:.4f}\".format(f1_average)\n",
        "# #     print(formatted_f1_average)\n",
        "    \n",
        "# #     return formatted_f1_average\n",
        "\n",
        "# # Split data into training and test sets\n",
        "# X_train=train_data_noID.drop('shop_profile', axis=1)\n",
        "# y_train=train_data_noID['shop_profile']\n",
        "# X_test=test_data_noID.drop('shop_profile', axis=1)\n",
        "# y_test=test_data_noID['shop_profile']\n",
        "\n",
        "# # Define the models to be hyperparameter tuned\n",
        "# models = [    {'name': 'XGBoost',        \n",
        "#             'model': XGBClassifier(tree_method='gpu_hist'),        \n",
        "#             'params': {\n",
        "#             'n_estimators': [50, 100, 200],\n",
        "#             'max_depth': [3, 5, 7],\n",
        "#             'learning_rate': [0.01, 0.1, 0.3],\n",
        "#             'min_child_weight': [1, 3, 5],\n",
        "#             'gamma': [0.0, 0.1, 0.2]\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         'name': 'Random Forest',\n",
        "#         'model': RandomForestClassifier(),\n",
        "#         'params': {\n",
        "#             'n_estimators': [50, 100, 200],\n",
        "#             'max_depth': [3, 5, 7],\n",
        "#             'min_samples_split': [2, 5, 10],\n",
        "#             'min_samples_leaf': [1, 2, 4],\n",
        "#             'max_features': ['sqrt', 'log2']\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         'name': 'Logistic Regression',\n",
        "#         'model': LogisticRegression(penalty='l2'),\n",
        "#         'params': {\n",
        "#             'C': [0.1, 1.0, 10.0],\n",
        "#             'solver': ['lbfgs', 'liblinear', 'saga'],\n",
        "#             'max_iter': [100, 500, 1000]\n",
        "#         }\n",
        "#     },\n",
        "#     {\n",
        "#         'name': 'KNN',\n",
        "#         'model': KNeighborsClassifier(),\n",
        "#         'params': {\n",
        "#             'n_neighbors': [3, 5, 7],\n",
        "#             'p': [1, 2],\n",
        "#             'weights': ['uniform', 'distance'],\n",
        "#             'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
        "#         }\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "\n",
        "# # Perform hyperparameter tuning for each model using GridSearchCV\n",
        "# best_model = None\n",
        "# best_score = 0.0\n",
        "# for model_config in models:\n",
        "#     print(f'Tuning {model_config[\"name\"]}...')\n",
        "#     model = model_config['model']\n",
        "#     params = model_config['params']\n",
        "#     # Create scorer object using custom scoring function\n",
        "#     # scorer = make_scorer(custom_scorer)\n",
        "\n",
        "#     custom_grid_search = GridSearchCV(\n",
        "#         estimator=model,\n",
        "#         param_grid=params,\n",
        "#         cv=5,\n",
        "#         # scoring=scorer,\n",
        "#         n_jobs=-1\n",
        "#     )\n",
        "#     custom_grid_search.fit(X_train, y_train)\n",
        "#     score = custom_grid_search.best_score_\n",
        "#     print(f'Best score for {model_config[\"name\"]}: {score:.4f}')\n",
        "#     print(f'Best Parametersfor {model_config[\"name\"]}:  {custom_grid_search.best_params_}')\n",
        "#     if score > best_score:\n",
        "#         best_score = score\n",
        "#         best_model = custom_grid_search.best_estimator_\n",
        "#         best_model_name = model_config['name']\n",
        "#         best_model_hyperparams = custom_grid_search.best_params_\n",
        "\n",
        "# # Train the best model on the full training set\n",
        "# best_model.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions on the test set\n",
        "# y_pred = best_model.predict(X_test)\n",
        "\n",
        "# # Calculate accuracy score on the test set\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f'Test accuracy score for the best model: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_data_noID['shop_profile'] = train_data_noID['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "# test_data_noID['shop_profile'] = test_data_noID['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "# test_data['shop_profile'] = test_data['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "# train_data['shop_profile'] = train_data['shop_profile'].replace({0: 1, 1: 2, 2: 3})\n",
        "# y_pred['shop_profile'] = y_pred['shop_profile'].replace({0: 1, 1: 2, 2: 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tuning XGBoost...\n",
        "# Best score for XGBoost: 0.5472\n",
        "# Best Parametersfor XGBoost:  {'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 50}\n",
        "# Tuning Random Forest...\n",
        "# Best score for Random Forest: 0.6070\n",
        "# Best Parametersfor Random Forest:  {'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 150}\n",
        "# Tuning Logistic Regression...\n",
        "# Best score for Logistic Regression: 0.5213\n",
        "# Best Parametersfor Logistic Regression:  {'C': 10.0, 'max_iter': 100, 'solver': 'liblinear'}\n",
        "# Tuning KNN...\n",
        "# Best score for KNN: 0.4415\n",
        "# Best Parametersfor KNN:  {'algorithm': 'auto', 'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}\n",
        "# Test accuracy score for the best model: 0.7184"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
