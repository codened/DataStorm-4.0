{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawHisTransDF=pd.read_csv('Historical-transaction-data.csv')\n",
    "rawStoreInfDF=pd.read_csv('Store-info.csv')\n",
    "rawTestDF=pd.read_csv('Testing-data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawHisTransDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawStoreInfDF.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date string column to datetime\n",
    "rawHisTransDF['transaction_date'] = pd.to_datetime(rawHisTransDF['transaction_date'], format='%Y/%m/%d').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of null values in each column\n",
    "null_counts = rawHisTransDF.isnull().sum()\n",
    "# print the counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawHisTransDF.dropna(subset=['item_description','invoice_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of null values in each column\n",
    "null_counts = rawHisTransDF.isnull().sum()\n",
    "# print the counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawHisTransDF=rawHisTransDF.drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "rawHisTransDF['item_description'] = le.fit_transform(rawHisTransDF['item_description'])\n",
    "rawHisTransDF['customer_id'] = le.fit_transform(rawHisTransDF['customer_id'])\n",
    "rawHisTransDF['shop_id'] = rawHisTransDF['shop_id'].str.replace(r'^SHOP', '').astype(int)\n",
    "rawStoreInfDF['shop_id'] = rawStoreInfDF['shop_id'].str.replace(r'^SHOP', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawStoreInfDF['shop_profile'] = rawStoreInfDF['shop_profile'].replace({'High': 1, 'Moderate': 2, 'Low': 3})\n",
    "rawStoreInfDF['shop_profile'] = rawStoreInfDF['shop_profile'].fillna(0.0).astype(int)\n",
    "rawHisTransDF['invoice_id'] = rawHisTransDF['invoice_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawStoreInfDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawHisTransDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the dataframe by the 'group' column and get the size of each group\n",
    "transactions_by_shop = rawHisTransDF.groupby('shop_id').size().reset_index()\n",
    "\n",
    "# rename columns of the new dataframe\n",
    "transactions_by_shop.columns = ['shop_id', 'num_of_transactions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawStoreInfDF['transaction_by_shop']=transactions_by_shop\n",
    "rawStoreInfDF = pd.merge(rawStoreInfDF, transactions_by_shop, on='shop_id')\n",
    "rawStoreInfDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of null values in each column\n",
    "null_counts = rawStoreInfDF.isnull().sum()\n",
    "# print the counts\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[['shop_id', 'Daily_Sales_avg', 'revnew', 'rev_per_sqfeet', 'avd_daily_items_types_sold', 'avd_daily_transctions', 'avd_daily_custemers', 'avg_visits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output =output.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawStoreInfDF['transaction_by_shop']=transactions_by_shop\n",
    "rawStoreInfDF = pd.merge(rawStoreInfDF, output, on='shop_id')\n",
    "rawStoreInfDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawStoreInfDF=rawStoreInfDF.drop(['avd_daily_transctions','revnew','avd_daily_custemers','num_of_transactions'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load your data into a pandas dataframe\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# compute the correlation matrix\n",
    "corr_matrix = rawStoreInfDF.corr()\n",
    "\n",
    "# plot the correlation matrix as a heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into two based on column B\n",
    "TestDF = rawStoreInfDF[rawStoreInfDF['shop_profile'] == 0].drop(['shop_profile'], axis=1)\n",
    "TrainDF = rawStoreInfDF[rawStoreInfDF['shop_profile'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Fulldata into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "column_name = 'shop_id'\n",
    "unique_categories = TrainDF[column_name].nunique()\n",
    "categories_in_dataset_1 = int(unique_categories * 0.8)\n",
    "categories_in_dataset_2 = unique_categories - categories_in_dataset_1\n",
    "dataset_1_categories = TrainDF[column_name].unique()[:categories_in_dataset_1]\n",
    "dataset_2_categories = TrainDF[column_name].unique()[categories_in_dataset_1:]\n",
    "\n",
    "train_data = TrainDF[TrainDF[column_name].isin(dataset_1_categories)]\n",
    "test_data = TrainDF[TrainDF[column_name].isin(dataset_2_categories)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_data, test_data = train_test_split(TrainDF, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('gpttdanna.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedResult=test_data[['shop_id','shop_profile']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # group the dataframe by the 'group' column\n",
    "# grouped = expectedResult.groupby('shop_id')\n",
    "\n",
    "# # find the mode value of each group\n",
    "# TestMode_df = grouped['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()\n",
    "\n",
    "# # rename columns of the new dataframe\n",
    "# TestMode_df.columns = ['shop_id', 'shop_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TestMode_df['shop_id']=TestMode_df['shop_id'].astype(int)\n",
    "# TestMode_df['shop_profile']=TestMode_df['shop_profile'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_x_test = test_data['shop_id']\n",
    "shop_id_x_TestDF = TestDF['shop_id']\n",
    "TestDF=TestDF.drop('shop_id',axis=1)\n",
    "train_data=train_data.drop('shop_id', axis=1)\n",
    "test_data=test_data.drop('shop_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_data.drop(['shop_profile'], axis=1)\n",
    "y_train=train_data['shop_profile']\n",
    "X_test= test_data.drop(['shop_profile'], axis=1)\n",
    "y_test=test_data['shop_profile']\n",
    "X_testres = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Create an instance of RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "temp_X_train=X_train\n",
    "temp_x_test=X_test\n",
    "temp_TestDF=TestDF\n",
    "# Scale the data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "TestDF = scaler.fit_transform(TestDF)\n",
    "\n",
    "# Convert the scaled data back to a pandas dataframe\n",
    "X_train = pd.DataFrame(X_train, columns=temp_X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=temp_x_test.columns)\n",
    "TestDF = pd.DataFrame(TestDF, columns=temp_TestDF.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the dataset using the scaler\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "TestDF = scaler.transform(TestDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the logistic regression model\n",
    "model_random = RandomForestClassifier(max_depth=25, min_samples_leaf=1, min_samples_split=2, n_estimators=10)\n",
    "\n",
    "# Train the model on the training data\n",
    "model_random.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = model_random.predict(X_test)\n",
    "predictions_Test_randomforest=model_random.predict(TestDF)\n",
    "\n",
    "accu = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(accu)\n",
    "# print(f1_score(y_test, predictions, average=None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_Test_randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf=pd.DataFrame(predictions, columns=['shop_profile'])\n",
    "predDf_Test_randomforest=pd.DataFrame(predictions_Test_randomforest, columns=['shop_profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_x_TestDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_x_test=shop_id_x_test.reset_index()\n",
    "shop_id_x_TestDF=shop_id_x_TestDF.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_x_TestDF=shop_id_x_TestDF.drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate DataFrames\n",
    "concatenatedRes_df = pd.concat([shop_id_x_test, predDf], axis=1)\n",
    "# concatenatedRes_df = pd.concat([concatenatedRes_df, shop_id_x_test], axis=1)\n",
    "# concatenated_df = pd.concat([shop_id_x_test, predDf], ignore_index=True)\n",
    "concatenatedRes_df_random = pd.concat([shop_id_x_TestDF, predDf_Test_randomforest], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df_random['shop_profile'] = concatenatedRes_df_random['shop_profile'].replace({1: 'High', 2: 'Moderate', 3: 'Low'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df_random.to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # group the dataframe by the 'group' column\n",
    "# grouped = concatenatedRes_df.groupby('shop_id')\n",
    "\n",
    "# # find the mode value of each group\n",
    "# result = grouped['shop_profile'].apply(lambda x: x.mode()[0]).reset_index()\n",
    "\n",
    "# # rename columns of the new dataframe\n",
    "# result.columns = ['shop_id', 'shop_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result['shop_id']=result['shop_id'].astype(int)\n",
    "# result['shop_profile']=result['shop_profile'].astype(int)\n",
    "# result=result['shop_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedResult=expectedResult['shop_profile']\n",
    "concatenatedRes_df=concatenatedRes_df['shop_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each class\n",
    "f1_class0 = f1_score(expectedResult, concatenatedRes_df, labels=[1], average='weighted')\n",
    "f1_class1 = f1_score(expectedResult, concatenatedRes_df, labels=[2], average='weighted')\n",
    "f1_class2 = f1_score(expectedResult, concatenatedRes_df, labels=[3], average='weighted')\n",
    "\n",
    "# Calculate average F1 score\n",
    "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
    "\n",
    "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
    "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
    "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
    "print(f\"Average F1 score: {f1_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix = confusion_matrix(expectedResult, concatenatedRes_df)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace({1: 0, 2: 1, 3: 2})\n",
    "y_test = y_test.replace({1: 0, 2: 1, 3: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# define the XGBoost model\n",
    "model_xg = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "\n",
    "# train the model on the training data\n",
    "model_xg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = model_xg.predict(X_test)\n",
    "y_pred_Test_xgb=model_xg.predict(TestDF)\n",
    "\n",
    "# calculate the accuracy score of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf_xg=pd.DataFrame(y_pred, columns=['shop_profile'])\n",
    "predDf_xg_Test=pd.DataFrame(y_pred_Test_xgb, columns=['shop_profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate DataFrames\n",
    "concatenatedRes_df_xg = pd.concat([shop_id_x_test, predDf_xg], axis=1)\n",
    "# concatenatedRes_df = pd.concat([concatenatedRes_df, shop_id_x_test], axis=1)\n",
    "# concatenated_df = pd.concat([shop_id_x_test, predDf], ignore_index=True)\n",
    "concatenatedRes_df_xg_Test = pd.concat([shop_id_x_TestDF, predDf_xg_Test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df_xg_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df_xg_Test['shop_profile'] = concatenatedRes_df_xg_Test['shop_profile'].replace({0: 'High', 1: 'Moderate', 2: 'Low'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.concat([final,concatenatedRes_df_xg_Test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expectedResult=expectedResult['shop_profile']\n",
    "concatenatedRes_df_xg=concatenatedRes_df_xg['shop_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace({0: 1, 1: 2, 2: 3})\n",
    "y_test = y_test.replace({0: 1, 1: 2, 2: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each class\n",
    "f1_class0 = f1_score(expectedResult, concatenatedRes_df_xg, labels=[1], average='weighted')\n",
    "f1_class1 = f1_score(expectedResult, concatenatedRes_df_xg, labels=[2], average='weighted')\n",
    "f1_class2 = f1_score(expectedResult, concatenatedRes_df_xg, labels=[3], average='weighted')\n",
    "\n",
    "# Calculate average F1 score\n",
    "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
    "\n",
    "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
    "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
    "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
    "print(f\"Average F1 score: {f1_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix = confusion_matrix(expectedResult, concatenatedRes_df_xg)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# define the KNN model\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# train the model on the training data\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "y_pred_knn_Test = model_knn.predict(TestDF)\n",
    "\n",
    "# calculate the accuracy score of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf_knn=pd.DataFrame(y_pred_knn, columns=['shop_profile'])\n",
    "predDf_knn_Test=pd.DataFrame(y_pred_knn_Test, columns=['shop_profile'])\n",
    "# Concatenate DataFrames\n",
    "concatenatedRes_df_knn = pd.concat([shop_id_x_test, predDf_knn], axis=1)\n",
    "concatenatedRes_df_knn_Test = pd.concat([shop_id_x_TestDF, predDf_knn_Test], axis=1)\n",
    "# concatenatedRes_df = pd.concat([concatenatedRes_df, shop_id_x_test], axis=1)\n",
    "# concatenated_df = pd.concat([shop_id_x_test, predDf], ignore_index=True)\n",
    "# expectedResult=expectedResult['shop_profile']\n",
    "concatenatedRes_df_knn=concatenatedRes_df_knn['shop_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df_knn_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df_knn_Test['shop_profile'] = concatenatedRes_df_knn_Test['shop_profile'].replace({1: 'High', 2: 'Moderate', 3: 'Low'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([final,concatenatedRes_df_knn_Test],axis=1)\n",
    "final.to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each class\n",
    "f1_class0 = f1_score(expectedResult, concatenatedRes_df_knn, labels=[1], average='weighted')\n",
    "f1_class1 = f1_score(expectedResult, concatenatedRes_df_knn, labels=[2], average='weighted')\n",
    "f1_class2 = f1_score(expectedResult, concatenatedRes_df_knn, labels=[3], average='weighted')\n",
    "\n",
    "# Calculate average F1 score\n",
    "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
    "\n",
    "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
    "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
    "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
    "print(f\"Average F1 score: {f1_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix = confusion_matrix(expectedResult, concatenatedRes_df_knn)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# define the KNN model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# train the model on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred_clf = model_knn.predict(X_test)\n",
    "y_pred_clf_Test = model_knn.predict(TestDF)\n",
    "\n",
    "# calculate the accuracy score of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred_clf)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf_clf=pd.DataFrame(y_pred_clf, columns=['shop_profile'])\n",
    "predDf_clf_Test=pd.DataFrame(y_pred_clf_Test, columns=['shop_profile'])\n",
    "# Concatenate DataFrames\n",
    "concatenatedRes_df_clf = pd.concat([shop_id_x_test, predDf_clf], axis=1)\n",
    "concatenatedRes_df_clf_Test = pd.concat([shop_id_x_TestDF, predDf_clf_Test], axis=1)\n",
    "# concatenatedRes_df = pd.concat([concatenatedRes_df, shop_id_x_test], axis=1)\n",
    "# concatenated_df = pd.concat([shop_id_x_test, predDf], ignore_index=True)\n",
    "# expectedResult=expectedResult['shop_profile']\n",
    "concatenatedRes_df_clf=concatenatedRes_df_clf['shop_profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenatedRes_df_clf_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each class\n",
    "f1_class0 = f1_score(expectedResult, concatenatedRes_df_clf, labels=[1], average='weighted')\n",
    "f1_class1 = f1_score(expectedResult, concatenatedRes_df_clf, labels=[2], average='weighted')\n",
    "f1_class2 = f1_score(expectedResult, concatenatedRes_df_clf, labels=[3], average='weighted')\n",
    "\n",
    "# Calculate average F1 score\n",
    "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
    "\n",
    "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
    "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
    "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
    "print(f\"Average F1 score: {f1_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix = confusion_matrix(expectedResult, concatenatedRes_df_clf)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # One-hot encode the target variable\n",
    "# enc = OneHotEncoder()\n",
    "# y_train = enc.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "# y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "# # Create a neural network model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "# model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model on the training data\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=10)\n",
    "\n",
    "# # Predict the classes of the testing data\n",
    "# y_pred = model.predict_classes(X_test)\n",
    "\n",
    "# # Decode one-hot encoded labels back to original labels\n",
    "# y_test = enc.inverse_transform(y_test)\n",
    "# y_pred = enc.inverse_transform(y_pred.reshape(-1, 1))\n",
    "\n",
    "# # Calculate the accuracy of the classifier\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# # Print the accuracy of the classifier\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Train the first set of models\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "model2 = RandomForestClassifier(max_depth=25, min_samples_leaf=1, min_samples_split=2, n_estimators=10)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using the first set of models\n",
    "y_pred_1 = model1.predict(X_test)\n",
    "y_pred_2 = model2.predict(X_test)\n",
    "y_pred_1_test = model1.predict(TestDF)\n",
    "y_pred_2_test = model2.predict(TestDF)\n",
    "\n",
    "# Stack the predictions from the first set of models\n",
    "X_stack = np.column_stack((y_pred_1, y_pred_2))\n",
    "X_stack_test = np.column_stack((y_pred_1_test,y_pred_2_test))\n",
    "\n",
    "# Train the final model on the stacked predictions\n",
    "final_model = LogisticRegression()\n",
    "final_model.fit(X_stack, y_test)\n",
    "\n",
    "# Make predictions on the testing set using the final model\n",
    "y_pred_int = final_model.predict(X_stack)\n",
    "y_pred_int_test = final_model.predict(X_stack_test)\n",
    "\n",
    "# Calculate the accuracy of the final predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf_int=pd.DataFrame(y_pred_int, columns=['shop_profile'])\n",
    "predDf_int_Test=pd.DataFrame(y_pred_int_test, columns=['shop_profile'])\n",
    "# Concatenate DataFrames\n",
    "concatenatedRes_df_int = pd.concat([shop_id_x_test, predDf_int], axis=1)\n",
    "concatenatedRes_df_int_Test = pd.concat([shop_id_x_TestDF, predDf_int_Test], axis=1)\n",
    "# concatenatedRes_df = pd.concat([concatenatedRes_df, shop_id_x_test], axis=1)\n",
    "# concatenated_df = pd.concat([shop_id_x_test, predDf], ignore_index=True)\n",
    "# expectedResult=expectedResult['shop_profile']\n",
    "concatenatedRes_df_int=concatenatedRes_df_int['shop_profile']\n",
    "concatenatedRes_df_int_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each class\n",
    "f1_class0 = f1_score(expectedResult, concatenatedRes_df_int, labels=[1], average='weighted')\n",
    "f1_class1 = f1_score(expectedResult, concatenatedRes_df_int, labels=[2], average='weighted')\n",
    "f1_class2 = f1_score(expectedResult, concatenatedRes_df_int, labels=[3], average='weighted')\n",
    "\n",
    "# Calculate average F1 score\n",
    "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
    "\n",
    "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
    "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
    "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
    "print(f\"Average F1 score: {f1_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix = confusion_matrix(expectedResult, concatenatedRes_df_int)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest and XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace({1: 0, 2: 1, 3: 2})\n",
    "y_test = y_test.replace({1: 0, 2: 1, 3: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Train the first set of models\n",
    "model1 = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "model2 = RandomForestClassifier(max_depth=25, min_samples_leaf=1, min_samples_split=2, n_estimators=10)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using the first set of models\n",
    "y_pred_1 = model1.predict(X_test)\n",
    "y_pred_2 = model2.predict(X_test)\n",
    "y_pred_1_test = model1.predict(TestDF)\n",
    "y_pred_2_test = model2.predict(TestDF)\n",
    "\n",
    "# Stack the predictions from the first set of models\n",
    "X_stack = np.column_stack((y_pred_1, y_pred_2))\n",
    "X_stack_test = np.column_stack((y_pred_1_test,y_pred_2_test))\n",
    "\n",
    "# Train the final model on the stacked predictions\n",
    "final_model = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "final_model.fit(X_stack, y_test)\n",
    "\n",
    "# Make predictions on the testing set using the final model\n",
    "y_pred_int = final_model.predict(X_stack)\n",
    "y_pred_int_test = final_model.predict(X_stack_test)\n",
    "\n",
    "# Calculate the accuracy of the final predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace({0: 1, 1: 2, 2: 3})\n",
    "y_test = y_test.replace({0: 1, 1: 2, 2: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf_int=pd.DataFrame(y_pred_int, columns=['shop_profile'])\n",
    "predDf_int_Test=pd.DataFrame(y_pred_int_test, columns=['shop_profile'])\n",
    "# Concatenate DataFrames\n",
    "concatenatedRes_df_int = pd.concat([shop_id_x_test, predDf_int], axis=1)\n",
    "concatenatedRes_df_int_Test = pd.concat([shop_id_x_TestDF, predDf_int_Test], axis=1)\n",
    "# concatenatedRes_df = pd.concat([concatenatedRes_df, shop_id_x_test], axis=1)\n",
    "# concatenated_df = pd.concat([shop_id_x_test, predDf], ignore_index=True)\n",
    "# expectedResult=expectedResult['shop_profile']\n",
    "concatenatedRes_df_int=concatenatedRes_df_int['shop_profile']\n",
    "concatenatedRes_df_int_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each class\n",
    "f1_class0 = f1_score(expectedResult, concatenatedRes_df_int, labels=[0], average='weighted')\n",
    "f1_class1 = f1_score(expectedResult, concatenatedRes_df_int, labels=[1], average='weighted')\n",
    "f1_class2 = f1_score(expectedResult, concatenatedRes_df_int, labels=[2], average='weighted')\n",
    "\n",
    "# Calculate average F1 score\n",
    "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
    "\n",
    "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
    "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
    "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
    "print(f\"Average F1 score: {f1_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix = confusion_matrix(expectedResult, concatenatedRes_df_int)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knn and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Train the first set of models\n",
    "model1 = KNeighborsClassifier(n_neighbors=10)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "model2 = RandomForestClassifier(max_depth=100, min_samples_leaf=5, min_samples_split=2, n_estimators=25)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using the first set of models\n",
    "y_pred_1 = model1.predict(X_test)\n",
    "y_pred_2 = model2.predict(X_test)\n",
    "y_pred_1_test = model1.predict(TestDF)\n",
    "y_pred_2_test = model2.predict(TestDF)\n",
    "\n",
    "# Stack the predictions from the first set of models\n",
    "X_stack = np.column_stack((y_pred_1, y_pred_2))\n",
    "X_stack_test = np.column_stack((y_pred_1_test,y_pred_2_test))\n",
    "\n",
    "# Train the final model on the stacked predictions\n",
    "final_model = KNeighborsClassifier(n_neighbors=5)\n",
    "final_model.fit(X_stack, y_test)\n",
    "\n",
    "# Make predictions on the testing set using the final model\n",
    "y_pred_int = final_model.predict(X_stack)\n",
    "y_pred_int_test = final_model.predict(X_stack_test)\n",
    "\n",
    "# Calculate the accuracy of the final predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf_int=pd.DataFrame(y_pred_int, columns=['shop_profile'])\n",
    "predDf_int_Test=pd.DataFrame(y_pred_int_test, columns=['shop_profile'])\n",
    "# Concatenate DataFrames\n",
    "concatenatedRes_df_int = pd.concat([shop_id_x_test, predDf_int], axis=1)\n",
    "concatenatedRes_df_int_Test = pd.concat([shop_id_x_TestDF, predDf_int_Test], axis=1)\n",
    "# concatenatedRes_df = pd.concat([concatenatedRes_df, shop_id_x_test], axis=1)\n",
    "# concatenated_df = pd.concat([shop_id_x_test, predDf], ignore_index=True)\n",
    "# expectedResult=expectedResult['shop_profile']\n",
    "concatenatedRes_df_int=concatenatedRes_df_int['shop_profile']\n",
    "concatenatedRes_df_int_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each class\n",
    "f1_class0 = f1_score(expectedResult, concatenatedRes_df_int, labels=[0], average='weighted')\n",
    "f1_class1 = f1_score(expectedResult, concatenatedRes_df_int, labels=[1], average='weighted')\n",
    "f1_class2 = f1_score(expectedResult, concatenatedRes_df_int, labels=[2], average='weighted')\n",
    "\n",
    "# Calculate average F1 score\n",
    "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
    "\n",
    "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
    "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
    "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
    "print(f\"Average F1 score: {f1_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "matrix = confusion_matrix(expectedResult, concatenatedRes_df_int)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(matrix, annot=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
