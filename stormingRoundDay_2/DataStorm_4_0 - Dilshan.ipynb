{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codened/DataStorm-4.0/blob/main/stormingRound/DataStorm_4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.dtreeg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx9iBCyQ2k9z"
      },
      "source": [
        "Path \n",
        "stormingRound/DataStorm_4_0.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wW7bJky-1JT"
      },
      "source": [
        "# Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79FVB5FI6Cs2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be7BxsqWn7E7"
      },
      "source": [
        "# Importing Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Oll2JxA5x9Y"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF=pd.read_csv('Historical-transaction-data.csv')\n",
        "rawStoreInfDF=pd.read_csv('Store-info.csv')\n",
        "rawTestDF=pd.read_csv('Testing-data.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Viewing Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtD8h2FiSA2r"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rawStoreInfDF.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pre Processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fixing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnrCJ0o-Sb8v"
      },
      "outputs": [],
      "source": [
        "# convert the date string column to datetime\n",
        "rawHisTransDF['transaction_date'] = pd.to_datetime(rawHisTransDF['transaction_date'], format='%Y/%m/%d').dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFqYaVSyjoBH"
      },
      "outputs": [],
      "source": [
        "# Performing left join\n",
        "merged_df = pd.merge(rawHisTransDF, rawStoreInfDF, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "Rb8HF5A2S9eg",
        "outputId": "498e142e-dc75-4bf8-f90a-fb5b8af35d0d"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF.describe(include='all').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get count of null values in each column\n",
        "null_counts = merged_df.isnull().sum()\n",
        "# print the counts\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df.dropna(subset=['item_description','invoice_id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get count of null values in each column\n",
        "null_counts = merged_df.isnull().sum()\n",
        "# print the counts\n",
        "print(null_counts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "merged_df['item_description'] = le.fit_transform(merged_df['item_description'])\n",
        "merged_df['customer_id'] = le.fit_transform(merged_df['customer_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['shop_id'] = merged_df['shop_id'].str.replace(r'^SHOP', '').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['shop_profile'] = merged_df['shop_profile'].replace({'High': 1, 'Moderate': 2, 'Low': 3})\n",
        "merged_df['shop_profile'] = merged_df['shop_profile'].fillna(0.0).astype(int)\n",
        "merged_df['invoice_id'] = merged_df['invoice_id'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(merged_df[merged_df['quantity_sold'] == 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = merged_df[merged_df['quantity_sold'] != 0]\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split To Test and Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the DataFrame into two based on column B\n",
        "TestDF = merged_df[merged_df['shop_profile'] == 0].drop(['shop_profile'], axis=1)\n",
        "TrainDF = merged_df[merged_df['shop_profile'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split Fulldata into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "column_name = 'shop_id'\n",
        "unique_categories = TrainDF[column_name].nunique()\n",
        "categories_in_dataset_1 = int(unique_categories * 0.8)\n",
        "categories_in_dataset_2 = unique_categories - categories_in_dataset_1\n",
        "dataset_1_categories = TrainDF[column_name].unique()[:categories_in_dataset_1]\n",
        "dataset_2_categories = TrainDF[column_name].unique()[categories_in_dataset_1:]\n",
        "\n",
        "train_data = TrainDF[TrainDF[column_name].isin(dataset_1_categories)]\n",
        "test_data = TrainDF[TrainDF[column_name].isin(dataset_2_categories)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#train_data, test_data = train_test_split(TrainDF, test_size=0.01)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "X_train=train_data.drop(['shop_profile','transaction_date'], axis=1)\n",
        "y_train=train_data['shop_profile']\n",
        "X_test= test_data.drop(['shop_profile','transaction_date'], axis=1)\n",
        "y_test=test_data['shop_profile']\n",
        "\n",
        "# Define the logistic regression model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "accu = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(accu)\n",
        "# print(f1_score(y_test, predictions, average=None))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "# Generate a random dataset\n",
        "# X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Define the grid of hyperparameters to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expectedResult=test_data[['shop_id','shop_profile']]\n",
        "print(expectedResult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_values = expectedResult['shop_id'].nunique()\n",
        "\n",
        "print(unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predDf=pd.DataFrame(predictions, columns=['shop_profile'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_testres = X_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_testres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate DataFrames\n",
        "concatenatedRes_df = pd.concat([X_testres, predDf], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenatedRes_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ResMode_df = concatenatedRes_df.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ResMode_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestMode_df = expectedResult.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestMode_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate F1 score for each class\n",
        "f1_class0 = f1_score(TestMode_df, ResMode_df, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(TestMode_df, ResMode_df, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(TestMode_df, ResMode_df, labels=[3], average='weighted')\n",
        "\n",
        "# Calculate average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "matrix = confusion_matrix(TestMode_df, ResMode_df)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions_logr = model.predict(X_test)\n",
        "\n",
        "accu = accuracy_score(y_test, predictions_logr)\n",
        "\n",
        "print(accu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predDf_logr=pd.DataFrame(predictions_logr, columns=['shop_profile'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate DataFrames\n",
        "concatenatedRes_df_logr = pd.concat([X_testres, predDf_logr], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ResMode_df_logr = concatenatedRes_df_logr.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate F1 score for each class\n",
        "f1_class0 = f1_score(TestMode_df, ResMode_df_logr, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(TestMode_df, ResMode_df_logr, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(TestMode_df, ResMode_df_logr, labels=[3], average='weighted')\n",
        "\n",
        "# Calculate average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "matrix = confusion_matrix(TestMode_df, ResMode_df_logr)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize and train model\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions_dtree = model.predict(X_test)\n",
        "\n",
        "accu = accuracy_score(y_test, predictions_dtree)\n",
        "\n",
        "print(accu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predDf_dtree=pd.DataFrame(predictions_dtree, columns=['shop_profile'])\n",
        "# Concatenate DataFrames\n",
        "concatenatedRes_df_dtree = pd.concat([X_testres, predDf_dtree], axis=1)\n",
        "ResMode_df_dtree = concatenatedRes_df_dtree.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "f1_class0 = f1_score(TestMode_df, ResMode_df_dtree, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(TestMode_df, ResMode_df_dtree, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(TestMode_df, ResMode_df_dtree, labels=[3], average='weighted')\n",
        "\n",
        "# Calculate average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "matrix = confusion_matrix(TestMode_df, ResMode_df_dtree)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Support Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.svm import SVC\n",
        "\n",
        "# # Initialize and train model\n",
        "# model = SVC(kernel='linear')\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions on the testing data\n",
        "# predictions_sv = model.predict(X_test)\n",
        "\n",
        "# accu = accuracy_score(y_test, predictions_sv)\n",
        "\n",
        "# print(accu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predDf_sv=pd.DataFrame(predictions_sv, columns=['shop_profile'])\n",
        "# # Concatenate DataFrames\n",
        "# concatenatedRes_df_sv = pd.concat([X_testres, predDf_sv], axis=1)\n",
        "# ResMode_df_sv = concatenatedRes_df_sv.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)\n",
        "\n",
        "# # Calculate F1 score for each class\n",
        "# f1_class0 = f1_score(TestMode_df, ResMode_df_sv, labels=[1], average='weighted')\n",
        "# f1_class1 = f1_score(TestMode_df, ResMode_df_sv, labels=[2], average='weighted')\n",
        "# f1_class2 = f1_score(TestMode_df, ResMode_df_sv, labels=[3], average='weighted')\n",
        "\n",
        "# # Calculate average F1 score\n",
        "# f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "# print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "# print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "# print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "# print(f\"Average F1 score: {f1_average:.2f}\")\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# # Create confusion matrix\n",
        "# matrix = confusion_matrix(TestMode_df, ResMode_df_sv)\n",
        "\n",
        "# # Visualize confusion matrix\n",
        "# sns.heatmap(matrix, annot=True)\n",
        "# plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "y_train_xg = y_train-1\n",
        "y_test_xg = y_test-1\n",
        "# Initialize and train model\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train_xg)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions_xg = model.predict(X_test)\n",
        "\n",
        "accu = accuracy_score(y_test_xg, predictions_xg)\n",
        "\n",
        "print(accu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predDf_xg=pd.DataFrame(predictions_xg, columns=['shop_profile'])\n",
        "# Concatenate DataFrames\n",
        "concatenatedRes_df_xg = pd.concat([X_testres, predDf_xg], axis=1)\n",
        "ResMode_df_xg = concatenatedRes_df_xg.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)\n",
        "\n",
        "TestMode_df_xg = TestMode_df-1\n",
        "# Calculate F1 score for each class\n",
        "f1_class0 = f1_score(TestMode_df_xg, ResMode_df_xg, labels=[0], average='weighted')\n",
        "f1_class1 = f1_score(TestMode_df_xg, ResMode_df_xg, labels=[1], average='weighted')\n",
        "f1_class2 = f1_score(TestMode_df_xg, ResMode_df_xg, labels=[2], average='weighted')\n",
        "\n",
        "# Calculate average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "matrix = confusion_matrix(TestMode_df, ResMode_df_xg)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict for test values\n",
        "Evalpredictions = model.predict(TestDF.drop(['transaction_date'], axis=1))\n",
        "\n",
        "EvalpredictionsDF=pd.DataFrame(Evalpredictions, columns=['shop_profile'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvalpredictionsDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestDFinReseted = TestDF.reset_index(drop=True)\n",
        "# Concatenate DataFrames\n",
        "concatenatedEval_df = pd.concat([TestDFinReseted['shop_id'], EvalpredictionsDF], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvalMode_df = (concatenatedEval_df.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)).to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvalMode_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save array to CSV file\n",
        "np.savetxt('EvalResult.csv', EvalMode_df, delimiter=',')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UploadShid=pd.read_csv('Testing-datatoUpload.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UploadShid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UploadShid['shop_profile'] = UploadShid['shop_profile'].replace({1 : 'High', 2 : 'Moderate',3 : 'Low'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UploadShid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save array to CSV file\n",
        "UploadShid.to_csv('Testing-datatoUpload.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Import necessary libraries\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# X_train=train_data.drop(['shop_profile','transaction_date'], axis=1)\n",
        "# y_train=train_data['shop_profile']\n",
        "# X_test= test_data.drop(['shop_profile','transaction_date'], axis=1)\n",
        "# y_test=test_data['shop_profile']\n",
        "\n",
        "# # Create a list of machine learning models to try out\n",
        "# models = []\n",
        "# models.append(('Logistic Regression', LogisticRegression()))\n",
        "# models.append(('SVM', SVC()))\n",
        "# models.append(('Decision Tree', DecisionTreeClassifier()))\n",
        "# models.append(('Random Forest', RandomForestClassifier()))\n",
        "# models.append(('AdaBoost', AdaBoostClassifier()))\n",
        "# models.append(('Extra Trees', ExtraTreesClassifier()))\n",
        "# models.append(('K-Nearest Neighbors', KNeighborsClassifier()))\n",
        "# models.append(('Gaussian Naive Bayes', GaussianNB()))\n",
        "# models.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\n",
        "# models.append(('Gradient Boosting', GradientBoostingClassifier()))\n",
        "\n",
        "\n",
        "# # Define the hyperparameters to tune for each model\n",
        "# params = {\n",
        "#     'Logistic Regression': {'C': [0.1, 1, 10]},\n",
        "#     'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
        "#     'Decision Tree': {'max_depth': [2, 4, 6]},\n",
        "#     'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [2, 4, 6]},    \n",
        "#     'AdaBoost': {'learning_rate': [0.1, 0.01], 'n_estimators': [100, 200, 300]},\n",
        "#     'Extra Trees': {'n_estimators': [100, 200, 300], 'max_depth': [2, 4, 6]},\n",
        "#     'K-Nearest Neighbors': {'n_neighbors': [3, 5, 7]},\n",
        "#     'Gaussian Naive Bayes': {},\n",
        "#     'Linear Discriminant Analysis': {'solver': ['svd', 'lsqr']},\n",
        "#     'Gradient Boosting': {'learning_rate': [0.1, 0.01], 'n_estimators': [100, 200, 300], 'max_depth': [2, 4, 6]}\n",
        "# }\n",
        "\n",
        "# # Train and evaluate each model with hyperparameter tuning\n",
        "# results = []\n",
        "# names = []\n",
        "\n",
        "# accuResults=[]\n",
        "\n",
        "# resultsxxx=[]\n",
        "\n",
        "# for name, model in models:\n",
        "#     param_grid = params[name]\n",
        "#     clf = GridSearchCV(model, param_grid, cv=5)\n",
        "#     clf.fit(X_train, y_train)  # Fit the GridSearchCV object to the training data\n",
        "#     cv_results = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "#     results.append(cv_results)\n",
        "#     names.append(name)\n",
        "\n",
        "#     y_pred = clf.predict(X_test)\n",
        "#     accu = accuracy_score(y_test, y_pred)\n",
        "#     accuResults.append(accu)\n",
        "\n",
        "#     resultsxxx.append(accu*cv_results)\n",
        "\n",
        "#     print(f'{name}: cv : {cv_results.mean()}')\n",
        "#     print(f'{name}: accu : {accu}')\n",
        "#     print(f'Best parameters: {clf.best_params_}')  # Print the best parameters inside the loop\n",
        "\n",
        "\n",
        "# # Select the best model based on mean cross-validation score\n",
        "# best_idx_cv = np.argmax([np.mean(r) for r in results])\n",
        "# best_model_cv = models[best_idx_cv][1]\n",
        "# print(f'Best model from cv mean: {names[best_idx_cv]}')\n",
        "\n",
        "# # Evaluate the best model on the test set\n",
        "# best_model_cv.fit(X_train, y_train)\n",
        "# y_pred = best_model_cv.predict(X_test)\n",
        "# cv_resultscv = cross_val_score(best_model_cv, X_train, y_train, cv=5)\n",
        "# print(f'CV : {cv_resultscv.mean()}')\n",
        "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "# # Create a confusion matrix to visualize the performance of the model\n",
        "# cm1 = confusion_matrix(y_test, y_pred)\n",
        "# sns.heatmap(cm1, annot=True, cmap='Blues', fmt='g')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('True')\n",
        "# plt.title(f'Confusion Matrix for {best_model_cv}')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# # Select the best model based on mean accuracy score\n",
        "# best_idx_acc = np.argmax(accuResults)\n",
        "# best_model_acc = models[best_idx_acc][1]\n",
        "# print(f'Best model from accue Accuracy: {names[best_idx_acc]}')\n",
        "\n",
        "# # Evaluate the best model on the test set\n",
        "# best_model_acc.fit(X_train, y_train)\n",
        "# y_pred = best_model_acc.predict(X_test)\n",
        "# cv_resultsAcc = cross_val_score(best_model_acc, X_train, y_train, cv=5)\n",
        "# print(f'CV : {cv_resultsAcc.mean()}')\n",
        "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "# # Create a confusion matrix to visualize the performance of the model\n",
        "# cm2 = confusion_matrix(y_test, y_pred)\n",
        "# sns.heatmap(cm2, annot=True, cmap='Blues', fmt='g')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('True')\n",
        "# plt.title(f'Confusion Matrix for {best_model_acc}')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Select the best model based on mean multiplication\n",
        "# best_id_mul = np.argmax(resultsxxx)\n",
        "# best_model_mul = models[best_id_mul][1]\n",
        "# print(f'Best model from multiplication of two: {names[best_id_mul]}')\n",
        "\n",
        "# # Evaluate the best model on the test set\n",
        "# best_model_mul.fit(X_train, y_train)\n",
        "# y_pred = best_model_mul.predict(X_test)\n",
        "# cv_resultsAcc = cross_val_score(best_model_mul, X_train, y_train, cv=5)\n",
        "# print(f'CV : {cv_resultsAcc.mean()}')\n",
        "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "# print(f'Mul : {cv_resultsAcc.mean()*accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "# # Create a confusion matrix to visualize the performance of the model\n",
        "# cm3 = confusion_matrix(y_test, y_pred)\n",
        "# sns.heatmap(cm3, annot=True, cmap='Blues', fmt='g')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('True')\n",
        "# plt.title(f'Confusion Matrix for {best_model_mul}')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
