{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codened/DataStorm-4.0/blob/main/stormingRound/DataStorm_4_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.dtreeg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx9iBCyQ2k9z"
      },
      "source": [
        "Path \n",
        "stormingRound/DataStorm_4_0.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wW7bJky-1JT"
      },
      "source": [
        "# Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79FVB5FI6Cs2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be7BxsqWn7E7"
      },
      "source": [
        "# Importing Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Oll2JxA5x9Y"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF=pd.read_csv('Historical-transaction-data.csv')\n",
        "rawStoreInfDF=pd.read_csv('Store-info.csv')\n",
        "rawTestDF=pd.read_csv('Testing-data.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Viewing Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtD8h2FiSA2r"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rawStoreInfDF.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Pre Processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fixing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnrCJ0o-Sb8v"
      },
      "outputs": [],
      "source": [
        "# convert the date string column to datetime\n",
        "rawHisTransDF['transaction_date'] = pd.to_datetime(rawHisTransDF['transaction_date'], format='%Y/%m/%d').dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFqYaVSyjoBH"
      },
      "outputs": [],
      "source": [
        "# Performing left join\n",
        "merged_df = pd.merge(rawHisTransDF, rawStoreInfDF, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "Rb8HF5A2S9eg",
        "outputId": "498e142e-dc75-4bf8-f90a-fb5b8af35d0d"
      },
      "outputs": [],
      "source": [
        "rawHisTransDF.describe(include='all').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get count of null values in each column\n",
        "null_counts = merged_df.isnull().sum()\n",
        "# print the counts\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df.dropna(subset=['item_description','invoice_id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get count of null values in each column\n",
        "null_counts = merged_df.isnull().sum()\n",
        "# print the counts\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "merged_df['item_description'] = le.fit_transform(merged_df['item_description'])\n",
        "merged_df['customer_id'] = le.fit_transform(merged_df['customer_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['shop_id'] = merged_df['shop_id'].str.replace(r'^SHOP', '').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['shop_profile'] = merged_df['shop_profile'].replace({'High': 0, 'Moderate': 1, 'Low': 2})\n",
        "merged_df['shop_profile'] = merged_df['shop_profile'].fillna(0.0).astype(int)\n",
        "merged_df['invoice_id'] = merged_df['invoice_id'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(merged_df[merged_df['quantity_sold'] == 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = merged_df[merged_df['quantity_sold'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['full_price'] = merged_df['quantity_sold'] * merged_df['item_price']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### creating Avarage daily sales for each shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['Daily_Sales'] = merged_df.groupby(['shop_id', 'transaction_date'])['full_price'].transform('sum')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subset = merged_df.loc[(merged_df['transaction_date'] == pd.to_datetime('2021-12-11')) & (merged_df['shop_id'] == 8)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by shop id and calculate mean of daily_sales column\n",
        "avg_sales = merged_df.groupby('shop_id')['Daily_Sales'].mean().reset_index()\n",
        "\n",
        "# Merge the average sales data back into the original dataframe\n",
        "merged_df = merged_df.merge(avg_sales, on='shop_id', suffixes=('', '_avg'))\n",
        "\n",
        "# Print the updated dataframe\n",
        "merged_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Full revinew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['revnew'] = merged_df.groupby(['shop_id'])['full_price'].transform('sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Revnew per sqr feet of land"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['rev_per_sqfeet'] = (merged_df['revnew'] / merged_df['shop_area_sq_ft']).round().astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avarage sold item types per each shop "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# group the original table by Shop ID and Transaction Date and count the unique items sold on each day\n",
        "daily_items_sold = merged_df.groupby(['shop_id', 'transaction_date'])['item_description'].nunique().reset_index()\n",
        "\n",
        "# group the resulting table by Shop ID and take the mean of the nunique column\n",
        "avg_daily_items_sold = daily_items_sold.groupby('shop_id')['item_description'].mean().reset_index()\n",
        "\n",
        "# rename the columns\n",
        "avg_daily_items_sold.columns = ['shop_id', 'avd_daily_items_types_sold']\n",
        "# convert float column to integers\n",
        "avg_daily_items_sold['avd_daily_items_types_sold'] = avg_daily_items_sold['avd_daily_items_types_sold'].round().astype(int)\n",
        "\n",
        "# merge with the original dataframe\n",
        "merged_df = pd.merge(merged_df, avg_daily_items_sold, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avarage Daily Transactions per each shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# group the original table by Shop ID and Transaction Date and count the unique items sold on each day\n",
        "daily_trans = merged_df.groupby(['shop_id', 'transaction_date'])['invoice_id'].nunique().reset_index()\n",
        "\n",
        "# group the resulting table by Shop ID and take the mean of the nunique column\n",
        "avg_daily_trans = daily_trans.groupby('shop_id')['invoice_id'].mean().reset_index()\n",
        "\n",
        "# rename the columns\n",
        "avg_daily_trans.columns = ['shop_id', 'avd_daily_transctions']\n",
        "# convert float column to integers\n",
        "avg_daily_trans['avd_daily_transctions'] = avg_daily_trans['avd_daily_transctions'].round().astype(int)\n",
        "\n",
        "# merge with the original dataframe\n",
        "merged_df = pd.merge(merged_df, avg_daily_trans, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Average number of custemers per day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# group the original table by Shop ID and Transaction Date and count the unique items sold on each day\n",
        "daily_custemers = merged_df.groupby(['shop_id', 'transaction_date'])['customer_id'].nunique().reset_index()\n",
        "\n",
        "# group the resulting table by Shop ID and take the mean of the nunique column\n",
        "avg_daily_custemers = daily_custemers.groupby('shop_id')['customer_id'].mean().reset_index()\n",
        "\n",
        "# rename the columns\n",
        "avg_daily_custemers.columns = ['shop_id', 'avd_daily_custemers']\n",
        "# convert float column to integers\n",
        "avg_daily_custemers['avd_daily_custemers'] = avg_daily_custemers['avd_daily_custemers'].round().astype(int)\n",
        "\n",
        "# merge with the original dataframe\n",
        "merged_df = pd.merge(merged_df, avg_daily_custemers, on='shop_id', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Persentage of Avarage number of time the same customer returning for the same shop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the number of times each customer visited each shop\n",
        "visits = merged_df.groupby(['customer_id', 'shop_id'])['transaction_date'].count()\n",
        "# calculate the average number of visits per customer per shop\n",
        "avg_visits = visits.groupby(['shop_id']).mean()*100\n",
        "\n",
        "avg_visits=avg_visits.round().astype(int)\n",
        "# create a new DataFrame with the average visits\n",
        "avg_visits_df = avg_visits.reset_index().rename(columns={'transaction_date': 'avg_visits'})\n",
        "\n",
        "# merge the new DataFrame with the original DataFrame to add the average visits column\n",
        "merged_df = pd.merge(merged_df, avg_visits_df, on=['shop_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "corr = merged_df.corr()\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot correlation matrix as heatmap\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "\n",
        "# Display plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop highly co related features\n",
        "cleanedDF = merged_df.drop(['avd_daily_custemers','transaction_date','revnew','item_price','item_description','quantity_sold','full_price','customer_id'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "corr = cleanedDF.corr()\n",
        "\n",
        "# # Set figure size\n",
        "# plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot correlation matrix as heatmap\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "\n",
        "# Display plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split To Test and Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the DataFrame into two based on column B\n",
        "TestDF = cleanedDF[cleanedDF['shop_profile'] == 0].drop(['shop_profile'], axis=1)\n",
        "TrainDF = cleanedDF[cleanedDF['shop_profile'] != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Separate the target variable\n",
        "X = TrainDF.drop(['shop_profile'], axis=1)\n",
        "y = TrainDF['shop_profile']\n",
        "\n",
        "# Compute MI scores\n",
        "mi_scores = mutual_info_classif(X, y)\n",
        "\n",
        "# Convert to DataFrame and sort by MI score\n",
        "mi_scores_df = pd.DataFrame({'feature': X.columns, 'mi_score': mi_scores})\n",
        "mi_scores_df = mi_scores_df.sort_values('mi_score', ascending=False)\n",
        "\n",
        "# Plot bar chart of MI scores\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(mi_scores_df['feature'], mi_scores_df['mi_score'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('MI Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split Fulldata into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "column_name = 'shop_id'\n",
        "unique_categories = TrainDF[column_name].nunique()\n",
        "categories_in_dataset_1 = int(unique_categories * 0.8)\n",
        "categories_in_dataset_2 = unique_categories - categories_in_dataset_1\n",
        "dataset_1_categories = TrainDF[column_name].unique()[:categories_in_dataset_1]\n",
        "dataset_2_categories = TrainDF[column_name].unique()[categories_in_dataset_1:]\n",
        "\n",
        "train_data = TrainDF[TrainDF[column_name].isin(dataset_1_categories)]\n",
        "test_data = TrainDF[TrainDF[column_name].isin(dataset_2_categories)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#train_data, test_data = train_test_split(TrainDF, test_size=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove store id from the training and testing sets\n",
        "\n",
        "train_data_noID = train_data.drop(['shop_id'], axis=1)\n",
        "test_data_noID = test_data.drop(['shop_id'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train=train_data_noID.drop('shop_profile', axis=1)\n",
        "y_train=train_data_noID['shop_profile']\n",
        "X_test=test_data_noID.drop('shop_profile', axis=1)\n",
        "y_test=test_data_noID['shop_profile']\n",
        "\n",
        "# Define hyperparameter search space\n",
        "params = {\n",
        "    'n_estimators': [100, 500, 1000],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'gamma': [0, 0.1, 0.5, 1],\n",
        "    'colsample_bytree': [0.5, 0.7, 1.0],\n",
        "    'subsample': [0.5, 0.7, 1.0],\n",
        "    'reg_alpha': [0, 0.1, 1],\n",
        "    'reg_lambda': [0, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Create XGBoost regressor\n",
        "xgb = XGBRegressor(tree_method='gpu_hist', gpu_id=0)\n",
        "\n",
        "# Perform randomized search over hyperparameters\n",
        "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=50, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters\n",
        "print(\"Best hyperparameters: \", random_search.best_params_)\n",
        "\n",
        "# Get predictions on test set using best model\n",
        "best_model = random_search.best_estimator_\n",
        "xg_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance on test set\n",
        "mse = mean_squared_error(y_test, xg_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE: \", rmse)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best hyperparameters:  {'subsample': 0.5, 'reg_lambda': 0, 'reg_alpha': 0, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 0.5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_XG_res_ = pd.concat([X_testres, predDf_dtree], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train=train_data.drop('shop_profile', axis=1)\n",
        "y_train=train_data['shop_profile']\n",
        "X_test=test_data.drop('shop_profile', axis=1)\n",
        "y_test=test_data['shop_profile']\n",
        "\n",
        "\n",
        "# define XGBoost model with best hyperparameters\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    subsample=0.5,\n",
        "    reg_lambda=0,\n",
        "    reg_alpha=0,\n",
        "    n_estimators=500,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.01,\n",
        "    gamma=0,\n",
        "    colsample_bytree=0.5,\n",
        "    gpu_id=0,\n",
        "    tree_method='gpu_hist' # using GPU acceleration\n",
        ")\n",
        "\n",
        "# fit model to training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on test data\n",
        "xg_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# evaluate performance of model\n",
        "mse = mean_squared_error(y_test, xg_pred)\n",
        "print('MSE:', mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# load the data and split into training and testing sets\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# train a decision tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# calculate the F1 score for each class\n",
        "f1_class0 = f1_score(y_test, y_pred, labels=[0], average='weighted')\n",
        "f1_class1 = f1_score(y_test, y_pred, labels=[1], average='weighted')\n",
        "f1_class2 = f1_score(y_test, y_pred, labels=[2], average='weighted')\n",
        "\n",
        "# calculate the average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "# print the results\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ResMode_df_XG"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "X_train=train_data.drop(['shop_profile'], axis=1)\n",
        "y_train=train_data['shop_profile']\n",
        "X_test= test_data.drop(['shop_profile'], axis=1)\n",
        "y_test=test_data['shop_profile']\n",
        "\n",
        "# Define the logistic regression model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "accu = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(accu)\n",
        "# print(f1_score(y_test, predictions, average=None))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "expectedResult=test_data[['shop_id','shop_profile']]\n",
        "print(expectedResult)\n",
        "unique_values = expectedResult['shop_id'].nunique()\n",
        "print(unique_values)\n",
        "\n",
        "predDf=pd.DataFrame(predictions, columns=['shop_profile'])\n",
        "X_testres = X_test.reset_index(drop=True)\n",
        "\n",
        "# Concatenate DataFrames\n",
        "concatenatedRes_df = pd.concat([X_testres, predDf], axis=1)\n",
        "\n",
        "ResMode_df = concatenatedRes_df.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)\n",
        "TestMode_df = expectedResult.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestMode_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate F1 score for each class\n",
        "f1_class0 = f1_score(TestMode_df, ResMode_df, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(TestMode_df, ResMode_df, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(TestMode_df, ResMode_df, labels=[3], average='weighted')\n",
        "\n",
        "# Calculate average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "matrix = confusion_matrix(TestMode_df, ResMode_df)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions_logr = model.predict(X_test)\n",
        "\n",
        "accu = accuracy_score(y_test, predictions_logr)\n",
        "\n",
        "print(accu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predDf_logr=pd.DataFrame(predictions_logr, columns=['shop_profile'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate DataFrames\n",
        "concatenatedRes_df_logr = pd.concat([X_testres, predDf_logr], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ResMode_df_logr = concatenatedRes_df_logr.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate F1 score for each class\n",
        "f1_class0 = f1_score(TestMode_df, ResMode_df_logr, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(TestMode_df, ResMode_df_logr, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(TestMode_df, ResMode_df_logr, labels=[3], average='weighted')\n",
        "\n",
        "# Calculate average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "matrix = confusion_matrix(TestMode_df, ResMode_df_logr)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize and train model\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions_dtree = model.predict(X_test)\n",
        "\n",
        "accu = accuracy_score(y_test, predictions_dtree)\n",
        "\n",
        "print(accu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predDf_dtree=pd.DataFrame(predictions_dtree, columns=['shop_profile'])\n",
        "# Concatenate DataFrames\n",
        "concatenatedRes_df_dtree = pd.concat([X_testres, predDf_dtree], axis=1)\n",
        "ResMode_df_dtree = concatenatedRes_df_dtree.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "f1_class0 = f1_score(TestMode_df, ResMode_df_dtree, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(TestMode_df, ResMode_df_dtree, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(TestMode_df, ResMode_df_dtree, labels=[3], average='weighted')\n",
        "\n",
        "# Calculate average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "matrix = confusion_matrix(TestMode_df, ResMode_df_dtree)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Support Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize and train model\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions_sv = model.predict(X_test)\n",
        "\n",
        "accu = accuracy_score(y_test, predictions_sv)\n",
        "\n",
        "print(accu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predDf_sv=pd.DataFrame(predictions_sv, columns=['shop_profile'])\n",
        "# Concatenate DataFrames\n",
        "concatenatedRes_df_sv = pd.concat([X_testres, predDf_sv], axis=1)\n",
        "ResMode_df_sv = concatenatedRes_df_sv.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)\n",
        "\n",
        "# Calculate F1 score for each class\n",
        "f1_class0 = f1_score(TestMode_df, ResMode_df_sv, labels=[1], average='weighted')\n",
        "f1_class1 = f1_score(TestMode_df, ResMode_df_sv, labels=[2], average='weighted')\n",
        "f1_class2 = f1_score(TestMode_df, ResMode_df_sv, labels=[3], average='weighted')\n",
        "\n",
        "# Calculate average F1 score\n",
        "f1_average = (f1_class0 + f1_class1 + f1_class2) / 3\n",
        "\n",
        "print(f\"F1 score for class 0: {f1_class0:.2f}\")\n",
        "print(f\"F1 score for class 1: {f1_class1:.2f}\")\n",
        "print(f\"F1 score for class 2: {f1_class2:.2f}\")\n",
        "print(f\"Average F1 score: {f1_average:.2f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "matrix = confusion_matrix(TestMode_df, ResMode_df_sv)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "sns.heatmap(matrix, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict for test values\n",
        "Evalpredictions = model.predict(TestDF.drop(['transaction_date'], axis=1))\n",
        "\n",
        "EvalpredictionsDF=pd.DataFrame(Evalpredictions, columns=['shop_profile'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvalpredictionsDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestDFinReseted = TestDF.reset_index(drop=True)\n",
        "# Concatenate DataFrames\n",
        "concatenatedEval_df = pd.concat([TestDFinReseted['shop_id'], EvalpredictionsDF], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvalMode_df = (concatenatedEval_df.groupby('shop_id')['shop_profile'].agg(pd.Series.mode)).to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvalMode_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save array to CSV file\n",
        "np.savetxt('EvalResult.csv', EvalMode_df, delimiter=',')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UploadShid=pd.read_csv('Testing-datatoUpload.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UploadShid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UploadShid['shop_profile'] = UploadShid['shop_profile'].replace({1 : 'High', 2 : 'Moderate',3 : 'Low'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "UploadShid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save array to CSV file\n",
        "UploadShid.to_csv('Testing-datatoUpload.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Import necessary libraries\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# X_train=train_data.drop(['shop_profile','transaction_date'], axis=1)\n",
        "# y_train=train_data['shop_profile']\n",
        "# X_test= test_data.drop(['shop_profile','transaction_date'], axis=1)\n",
        "# y_test=test_data['shop_profile']\n",
        "\n",
        "# # Create a list of machine learning models to try out\n",
        "# models = []\n",
        "# models.append(('Logistic Regression', LogisticRegression()))\n",
        "# models.append(('SVM', SVC()))\n",
        "# models.append(('Decision Tree', DecisionTreeClassifier()))\n",
        "# models.append(('Random Forest', RandomForestClassifier()))\n",
        "# models.append(('AdaBoost', AdaBoostClassifier()))\n",
        "# models.append(('Extra Trees', ExtraTreesClassifier()))\n",
        "# models.append(('K-Nearest Neighbors', KNeighborsClassifier()))\n",
        "# models.append(('Gaussian Naive Bayes', GaussianNB()))\n",
        "# models.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\n",
        "# models.append(('Gradient Boosting', GradientBoostingClassifier()))\n",
        "\n",
        "\n",
        "# # Define the hyperparameters to tune for each model\n",
        "# params = {\n",
        "#     'Logistic Regression': {'C': [0.1, 1, 10]},\n",
        "#     'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
        "#     'Decision Tree': {'max_depth': [2, 4, 6]},\n",
        "#     'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [2, 4, 6]},    \n",
        "#     'AdaBoost': {'learning_rate': [0.1, 0.01], 'n_estimators': [100, 200, 300]},\n",
        "#     'Extra Trees': {'n_estimators': [100, 200, 300], 'max_depth': [2, 4, 6]},\n",
        "#     'K-Nearest Neighbors': {'n_neighbors': [3, 5, 7]},\n",
        "#     'Gaussian Naive Bayes': {},\n",
        "#     'Linear Discriminant Analysis': {'solver': ['svd', 'lsqr']},\n",
        "#     'Gradient Boosting': {'learning_rate': [0.1, 0.01], 'n_estimators': [100, 200, 300], 'max_depth': [2, 4, 6]}\n",
        "# }\n",
        "\n",
        "# # Train and evaluate each model with hyperparameter tuning\n",
        "# results = []\n",
        "# names = []\n",
        "\n",
        "# accuResults=[]\n",
        "\n",
        "# resultsxxx=[]\n",
        "\n",
        "# for name, model in models:\n",
        "#     param_grid = params[name]\n",
        "#     clf = GridSearchCV(model, param_grid, cv=5)\n",
        "#     clf.fit(X_train, y_train)  # Fit the GridSearchCV object to the training data\n",
        "#     cv_results = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "#     results.append(cv_results)\n",
        "#     names.append(name)\n",
        "\n",
        "#     y_pred = clf.predict(X_test)\n",
        "#     accu = accuracy_score(y_test, y_pred)\n",
        "#     accuResults.append(accu)\n",
        "\n",
        "#     resultsxxx.append(accu*cv_results)\n",
        "\n",
        "#     print(f'{name}: cv : {cv_results.mean()}')\n",
        "#     print(f'{name}: accu : {accu}')\n",
        "#     print(f'Best parameters: {clf.best_params_}')  # Print the best parameters inside the loop\n",
        "\n",
        "\n",
        "# # Select the best model based on mean cross-validation score\n",
        "# best_idx_cv = np.argmax([np.mean(r) for r in results])\n",
        "# best_model_cv = models[best_idx_cv][1]\n",
        "# print(f'Best model from cv mean: {names[best_idx_cv]}')\n",
        "\n",
        "# # Evaluate the best model on the test set\n",
        "# best_model_cv.fit(X_train, y_train)\n",
        "# y_pred = best_model_cv.predict(X_test)\n",
        "# cv_resultscv = cross_val_score(best_model_cv, X_train, y_train, cv=5)\n",
        "# print(f'CV : {cv_resultscv.mean()}')\n",
        "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "# # Create a confusion matrix to visualize the performance of the model\n",
        "# cm1 = confusion_matrix(y_test, y_pred)\n",
        "# sns.heatmap(cm1, annot=True, cmap='Blues', fmt='g')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('True')\n",
        "# plt.title(f'Confusion Matrix for {best_model_cv}')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# # Select the best model based on mean accuracy score\n",
        "# best_idx_acc = np.argmax(accuResults)\n",
        "# best_model_acc = models[best_idx_acc][1]\n",
        "# print(f'Best model from accue Accuracy: {names[best_idx_acc]}')\n",
        "\n",
        "# # Evaluate the best model on the test set\n",
        "# best_model_acc.fit(X_train, y_train)\n",
        "# y_pred = best_model_acc.predict(X_test)\n",
        "# cv_resultsAcc = cross_val_score(best_model_acc, X_train, y_train, cv=5)\n",
        "# print(f'CV : {cv_resultsAcc.mean()}')\n",
        "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "# # Create a confusion matrix to visualize the performance of the model\n",
        "# cm2 = confusion_matrix(y_test, y_pred)\n",
        "# sns.heatmap(cm2, annot=True, cmap='Blues', fmt='g')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('True')\n",
        "# plt.title(f'Confusion Matrix for {best_model_acc}')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Select the best model based on mean multiplication\n",
        "# best_id_mul = np.argmax(resultsxxx)\n",
        "# best_model_mul = models[best_id_mul][1]\n",
        "# print(f'Best model from multiplication of two: {names[best_id_mul]}')\n",
        "\n",
        "# # Evaluate the best model on the test set\n",
        "# best_model_mul.fit(X_train, y_train)\n",
        "# y_pred = best_model_mul.predict(X_test)\n",
        "# cv_resultsAcc = cross_val_score(best_model_mul, X_train, y_train, cv=5)\n",
        "# print(f'CV : {cv_resultsAcc.mean()}')\n",
        "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "# print(f'Mul : {cv_resultsAcc.mean()*accuracy_score(y_test, y_pred)}')\n",
        "\n",
        "# # Create a confusion matrix to visualize the performance of the model\n",
        "# cm3 = confusion_matrix(y_test, y_pred)\n",
        "# sns.heatmap(cm3, annot=True, cmap='Blues', fmt='g')\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('True')\n",
        "# plt.title(f'Confusion Matrix for {best_model_mul}')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
